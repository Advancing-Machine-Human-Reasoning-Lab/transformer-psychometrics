{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3",
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "This code has several purposes:\n",
    "\n",
    "1. To convert the GLUE diagnostic set into a json format that can be feed directly into the LSTM\n",
    "\n",
    "2. To parse the output of the LSTM and extract the accuracy on the diagnostic\n",
    "\n",
    "3. To parse anli into a format which can be combined with snli+mnli"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "   ID    Subcategory name      Domain  \\\n",
       "0  36  Lexical entailment  Artificial   \n",
       "1  37  Lexical entailment  Artificial   \n",
       "2  38  Lexical entailment        News   \n",
       "3  39  Lexical entailment        News   \n",
       "4  40  Lexical entailment   Wikipedia   \n",
       "\n",
       "                                             Premise  \\\n",
       "0                              The water is too hot.   \n",
       "1                             The water is too cold.   \n",
       "2  Falcon Heavy is the largest rocket since NASA'...   \n",
       "3  Falcon Heavy is the smallest rocket since NASA...   \n",
       "4  Adenoiditis symptoms often persist for ten or ...   \n",
       "\n",
       "                                          Hypothesis          Label  \n",
       "0                             The water is too cold.  contradiction  \n",
       "1                              The water is too hot.  contradiction  \n",
       "2  Falcon Heavy is the smallest rocket since NASA...  contradiction  \n",
       "3  Falcon Heavy is the largest rocket since NASA'...  contradiction  \n",
       "4  Adenoiditis symptoms often pass within ten day...  contradiction  "
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>ID</th>\n      <th>Subcategory name</th>\n      <th>Domain</th>\n      <th>Premise</th>\n      <th>Hypothesis</th>\n      <th>Label</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>36</td>\n      <td>Lexical entailment</td>\n      <td>Artificial</td>\n      <td>The water is too hot.</td>\n      <td>The water is too cold.</td>\n      <td>contradiction</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>37</td>\n      <td>Lexical entailment</td>\n      <td>Artificial</td>\n      <td>The water is too cold.</td>\n      <td>The water is too hot.</td>\n      <td>contradiction</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>38</td>\n      <td>Lexical entailment</td>\n      <td>News</td>\n      <td>Falcon Heavy is the largest rocket since NASA'...</td>\n      <td>Falcon Heavy is the smallest rocket since NASA...</td>\n      <td>contradiction</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>39</td>\n      <td>Lexical entailment</td>\n      <td>News</td>\n      <td>Falcon Heavy is the smallest rocket since NASA...</td>\n      <td>Falcon Heavy is the largest rocket since NASA'...</td>\n      <td>contradiction</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>40</td>\n      <td>Lexical entailment</td>\n      <td>Wikipedia</td>\n      <td>Adenoiditis symptoms often persist for ten or ...</td>\n      <td>Adenoiditis symptoms often pass within ten day...</td>\n      <td>contradiction</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 4
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "from statistics import mean\n",
    "import json\n",
    "diag = pd.read_csv('./lstm/snli/snli/snli_1.0/BERT_Diagnostic_Set.csv')\n",
    "diag.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "outfile = open('GLUE_Diag_nliLike.jsonl','w+')\n",
    "for index, row in diag.iterrows():\n",
    "    js = {'sentence1':row['Premise'],'sentence2':row['Hypothesis'],'gold_label':row['Label']}\n",
    "    outfile.writelines(json.dumps(js))\n",
    "    outfile.write('\\n')\n",
    "\n",
    "outfile.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[1, 0, 0, 0, 1]\n0.5036945812807881\n"
     ]
    }
   ],
   "source": [
    "# given the json scores of the lstm, get the accuracy column and see how well it did overall\n",
    "\n",
    "with open('./snli/snli/snli_1.0/GLUE_Diag_nliLike-snli+mnli-1-0.001-0.2-1-300.jsonl','r') as f:\n",
    "    answers_in_diag_order = []\n",
    "    for line in f.readlines():\n",
    "        line = json.loads(line)\n",
    "        answers_in_diag_order.append(line['correct'])\n",
    "    \n",
    "    print(answers_in_diag_order[0:5])\n",
    "    print(mean(answers_in_diag_order))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parse the anli train set into a single file with the same format as snli+mnli\n",
    "r1_f = open('./snli/snli/snli_1.0/anli_train_R1.jsonl','r')\n",
    "r2_f = open('./snli/snli/snli_1.0/anli_train_R2.jsonl','r')\n",
    "r3_f = open('./snli/snli/snli_1.0/anli_train_R3.jsonl','r')\n",
    "\n",
    "dev_r1 = open('./snli/snli/snli_1.0/anli_dev_R1.jsonl','r')\n",
    "dev_r2 = open('./snli/snli/snli_1.0/anli_dev_R2.jsonl','r')\n",
    "dev_r3 = open('./snli/snli/snli_1.0/anli_dev_R3.jsonl','r')\n",
    "\n",
    "train_anli_combined = open('./snli/snli/snli_1.0/anli_train_combined_snliLike.jsonl','w+')\n",
    "dev_anli_combined = open('./snli/snli/snli_1.0/anli_dev_combined_snliLike.jsonl','w+')\n",
    "\n",
    "# parse each file separately, change the format to match snli, and write it out to the combined dataset\n",
    "for line in r1_f.readlines():\n",
    "    old_js = json.loads(line)\n",
    "    new_js = {}\n",
    "    new_js['sentence1'] = old_js['context']\n",
    "    new_js['sentence2'] = old_js['hypothesis']\n",
    "    if old_js['label'] == 'e':\n",
    "        new_js['gold_label'] = 'entailment'\n",
    "    elif old_js['label'] == 'c':\n",
    "        new_js['gold_label'] = 'contradiction'\n",
    "    elif old_js['label'] == 'n':\n",
    "        new_js['gold_label'] = 'neutral'\n",
    "    train_anli_combined.writelines(json.dumps(new_js))\n",
    "    train_anli_combined.writelines('\\n')\n",
    "\n",
    "for line in r2_f.readlines():\n",
    "    old_js = json.loads(line)\n",
    "    new_js = {}\n",
    "    new_js['sentence1'] = old_js['context']\n",
    "    new_js['sentence2'] = old_js['hypothesis']\n",
    "    if old_js['label'] == 'e':\n",
    "        new_js['gold_label'] = 'entailment'\n",
    "    elif old_js['label'] == 'c':\n",
    "        new_js['gold_label'] = 'contradiction'\n",
    "    elif old_js['label'] == 'n':\n",
    "        new_js['gold_label'] = 'neutral'\n",
    "    train_anli_combined.writelines(json.dumps(new_js))\n",
    "    train_anli_combined.writelines('\\n') \n",
    "\n",
    "for line in r3_f.readlines():\n",
    "    old_js = json.loads(line)\n",
    "    new_js = {}\n",
    "    new_js['sentence1'] = old_js['context']\n",
    "    new_js['sentence2'] = old_js['hypothesis']\n",
    "    if old_js['label'] == 'e':\n",
    "        new_js['gold_label'] = 'entailment'\n",
    "    elif old_js['label'] == 'c':\n",
    "        new_js['gold_label'] = 'contradiction'\n",
    "    elif old_js['label'] == 'n':\n",
    "        new_js['gold_label'] = 'neutral'\n",
    "    train_anli_combined.writelines(json.dumps(new_js))\n",
    "    train_anli_combined.writelines('\\n')\n",
    "\n",
    "# do the same for the dev set\n",
    "for line in dev_r1.readlines():\n",
    "    old_js = json.loads(line)\n",
    "    new_js = {}\n",
    "    new_js['sentence1'] = old_js['context']\n",
    "    new_js['sentence2'] = old_js['hypothesis']\n",
    "    if old_js['label'] == 'e':\n",
    "        new_js['gold_label'] = 'entailment'\n",
    "    elif old_js['label'] == 'c':\n",
    "        new_js['gold_label'] = 'contradiction'\n",
    "    elif old_js['label'] == 'n':\n",
    "        new_js['gold_label'] = 'neutral'\n",
    "    dev_anli_combined.writelines(json.dumps(new_js))\n",
    "    dev_anli_combined.writelines('\\n')\n",
    "\n",
    "for line in dev_r2.readlines():\n",
    "    old_js = json.loads(line)\n",
    "    new_js = {}\n",
    "    new_js['sentence1'] = old_js['context']\n",
    "    new_js['sentence2'] = old_js['hypothesis']\n",
    "    if old_js['label'] == 'e':\n",
    "        new_js['gold_label'] = 'entailment'\n",
    "    elif old_js['label'] == 'c':\n",
    "        new_js['gold_label'] = 'contradiction'\n",
    "    elif old_js['label'] == 'n':\n",
    "        new_js['gold_label'] = 'neutral'\n",
    "    dev_anli_combined.writelines(json.dumps(new_js))\n",
    "    dev_anli_combined.writelines('\\n')\n",
    "\n",
    "for line in dev_r3.readlines():\n",
    "    old_js = json.loads(line)\n",
    "    new_js = {}\n",
    "    new_js['sentence1'] = old_js['context']\n",
    "    new_js['sentence2'] = old_js['hypothesis']\n",
    "    if old_js['label'] == 'e':\n",
    "        new_js['gold_label'] = 'entailment'\n",
    "    elif old_js['label'] == 'c':\n",
    "        new_js['gold_label'] = 'contradiction'\n",
    "    elif old_js['label'] == 'n':\n",
    "        new_js['gold_label'] = 'neutral'\n",
    "    dev_anli_combined.writelines(json.dumps(new_js))\n",
    "    dev_anli_combined.writelines('\\n')\n",
    "\n",
    "train_anli_combined.close()\n",
    "dev_anli_combined.close()\n",
    "r1_f.close()\n",
    "r2_f.close()\n",
    "r3_f.close()\n",
    "dev_r1.close()\n",
    "dev_r2.close()\n",
    "dev_r3.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now using the comibined anli dataset, combine it with the snli+mnli dataset to get the snli+mnli+anli dataset\n",
    "train_snli_mnli_combined = open('./snli/snli/snli_1.0/snli_mnli_train_combined.jsonl','r')\n",
    "dev_snli_mnli_combined = open('./snli/snli/snli_1.0/snli_mnli_dev_combined.jsonl','r')\n",
    "train_anli_combined = open('./snli/snli/snli_1.0/anli_train_combined_snliLike.jsonl','r')\n",
    "dev_anli_combined = open('./snli/snli/snli_1.0/anli_dev_combined_snliLike.jsonl','r')\n",
    "\n",
    "train_snli_mnli_anli_combined = open('./snli/snli/snli_1.0/snli_mnli_anli_train_combined.jsonl','w+')\n",
    "dev_snli_mnli_anli_combined = open('./snli/snli/snli_1.0/snli_mnli_anli_dev_combined.jsonl','w+')\n",
    "\n",
    "# write out the combined train set\n",
    "for line in train_snli_mnli_combined.readlines():\n",
    "    line = json.loads(line)\n",
    "    train_snli_mnli_anli_combined.writelines(json.dumps(line))\n",
    "    train_snli_mnli_anli_combined.writelines('\\n')\n",
    "\n",
    "for line in train_anli_combined.readlines():\n",
    "    line = json.loads(line)\n",
    "    train_snli_mnli_anli_combined.writelines(json.dumps(line))\n",
    "    train_snli_mnli_anli_combined.writelines('\\n')\n",
    "\n",
    "# do the same for the dev set\n",
    "for line in dev_anli_combined.readlines():\n",
    "    line = json.loads(line)\n",
    "    dev_snli_mnli_anli_combined.writelines(json.dumps(line))\n",
    "    dev_snli_mnli_anli_combined.writelines('\\n')\n",
    "\n",
    "for line in dev_snli_mnli_combined.readlines():\n",
    "    line = json.loads(line)\n",
    "    dev_snli_mnli_anli_combined.writelines(json.dumps(line))\n",
    "    dev_snli_mnli_anli_combined.writelines('\\n')\n",
    "\n",
    "train_snli_mnli_anli_combined.close()\n",
    "dev_snli_mnli_anli_combined.close()\n",
    "train_snli_mnli_combined.close()\n",
    "dev_snli_mnli_combined.close()\n",
    "train_anli_combined.close()\n",
    "dev_anli_combined.close()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./snli/snli/snli_1.0/snli_mnli_anli_train_combined.jsonl','r') as f:\n",
    "    for line in f.readlines():\n",
    "        js = json.loads(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}